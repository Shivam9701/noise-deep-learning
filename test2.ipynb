{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"Data\"\n",
    "\n",
    "# Define the path to save the preprocessed data\n",
    "preprocessed_path = \"Preprocessed-Data\"\n",
    "\n",
    "# Define the sampling rate\n",
    "sampling_rate = 16000\n",
    "\n",
    "# Define the number of mel frequency bins\n",
    "n_mels = 128\n",
    "\n",
    "# Define the hop length\n",
    "hop_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 333)\n",
      "(128, 330)\n",
      "(128, 380)\n",
      "(128, 339)\n",
      "(128, 338)\n",
      "(128, 390)\n",
      "(128, 322)\n",
      "(128, 315)\n",
      "(128, 347)\n",
      "(128, 334)\n",
      "(128, 370)\n",
      "(128, 364)\n",
      "(128, 383)\n",
      "(128, 342)\n",
      "(128, 390)\n",
      "(128, 363)\n",
      "(128, 389)\n",
      "(128, 375)\n",
      "(128, 332)\n",
      "(128, 344)\n",
      "(128, 315)\n",
      "(128, 316)\n",
      "(128, 446)\n",
      "(128, 384)\n",
      "(128, 334)\n",
      "(128, 338)\n",
      "(128, 328)\n",
      "(128, 326)\n",
      "(128, 319)\n",
      "(128, 334)\n",
      "(128, 373)\n",
      "(128, 358)\n",
      "(128, 363)\n",
      "(128, 383)\n",
      "(128, 327)\n",
      "(128, 316)\n",
      "(128, 414)\n",
      "(128, 366)\n",
      "(128, 352)\n",
      "(128, 338)\n",
      "(128, 318)\n",
      "(128, 343)\n",
      "(128, 361)\n",
      "(128, 323)\n",
      "(128, 374)\n",
      "(128, 399)\n",
      "(128, 409)\n",
      "(128, 597)\n",
      "(128, 360)\n",
      "(128, 385)\n",
      "(128, 332)\n",
      "(128, 358)\n",
      "(128, 387)\n",
      "(128, 416)\n",
      "(128, 360)\n",
      "(128, 341)\n",
      "(128, 411)\n",
      "(128, 360)\n",
      "(128, 330)\n",
      "(128, 372)\n",
      "(128, 317)\n",
      "(128, 355)\n",
      "(128, 375)\n"
     ]
    }
   ],
   "source": [
    "# Loop over the clean speech files\n",
    "for filename in os.listdir(os.path.join(dataset_path, \"CleanSpeech_training\")):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(os.path.join(dataset_path, \"CleanSpeech_training\", filename), sr=sampling_rate)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(y = audio, sr=sampling_rate, n_mels=n_mels, hop_length=hop_length)\n",
    "\n",
    "    # Convert the mel spectrogram to decibels\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    print(mel_spec_db.shape)\n",
    "\n",
    "    # Save the mel spectrogram to disk\n",
    "    np.save(os.path.join(preprocessed_path, f\"{filename}.npy\"), mel_spec_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the noisy speech files\n",
    "for filename in os.listdir(os.path.join(dataset_path, \"NoisySpeech_training\")):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(os.path.join(dataset_path, \"NoisySpeech_training\", filename), sr=sampling_rate)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(y = audio, sr=sampling_rate, n_mels=n_mels, hop_length=hop_length)\n",
    "\n",
    "    # Convert the mel spectrogram to decibels\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Save the mel spectrogram to disk\n",
    "    np.save(os.path.join(preprocessed_path + '/NoisySpeech_training', f\"{filename}.npy\"), mel_spec_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the preprocessed data\n",
    "preprocessed_path = \"Preprocessed-Data\"\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Function to load preprocessed data and labels\n",
    "def load_data_labels(data_path, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(os.path.join(data_path, label)):\n",
    "        if (label == \"NoisySpeech_training\" and filename.split('_')[2] == '20.0' or label == \"CleanSpeech_training\") :\n",
    "\n",
    "            file_path = os.path.join(data_path, label, filename)\n",
    "            mel_spec_db = np.load(file_path)\n",
    "\n",
    "            mel_spec_db_resized = resize(mel_spec_db, (128,431), anti_aliasing=True)\n",
    "            \n",
    "            # Extract the label from the filename\n",
    "            if label == \"NoisySpeech_training\":\n",
    "                fn = filename.split('_')[-1]\n",
    "                sample_number = int(''.join(filter(str.isdigit, fn)))\n",
    "            else:\n",
    "                sample_number = int(''.join(filter(str.isdigit, filename)))\n",
    "\n",
    "    \n",
    "            # Add the data and label to the lists\n",
    "            data.append(mel_spec_db_resized)\n",
    "            labels.append(sample_number)  # Convert label to integer\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data:  63\n",
      "Clean labels:  63\n"
     ]
    }
   ],
   "source": [
    "# Load clean speech data and labels\n",
    "clean_data, clean_labels = load_data_labels(preprocessed_path, \"CleanSpeech_training\")\n",
    "\n",
    "print(\"Clean data: \", len(clean_data))\n",
    "print(\"Clean labels: \", len(clean_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy data:  63\n",
      "Noisy labels:  63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load noisy speech data and labels\n",
    "noisy_data, noisy_labels = load_data_labels(preprocessed_path, \"NoisySpeech_training\")\n",
    "\n",
    "print(\"Noisy data: \", len(noisy_data))\n",
    "print(\"Noisy labels: \", len(noisy_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "clean_labels_encoded = encoder.fit_transform(np.array(clean_labels).reshape(-1, 1))\n",
    "noisy_labels_encoded = encoder.fit_transform(np.array(noisy_labels).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate clean and noisy data along with their labels.\n",
    "all_data = np.concatenate((clean_data, noisy_data), axis=0)\n",
    "all_labels = np.concatenate((clean_labels_encoded, noisy_labels_encoded), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data and labels\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "all_data, all_labels = shuffle(all_data, all_labels, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    all_data, all_labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15411733 -0.13628708 -0.12317581 ... -0.11594322 -0.11928651\n",
      "  -0.12002021]\n",
      " [-0.13866602 -0.1237194  -0.12153348 ... -0.1162685  -0.11604972\n",
      "  -0.11476348]\n",
      " [-0.16813572 -0.15899122 -0.15530667 ... -0.1363196  -0.13454363\n",
      "  -0.12525561]\n",
      " ...\n",
      " [-0.3137255  -0.3137255  -0.3137255  ... -0.3137255  -0.3137255\n",
      "  -0.3137255 ]\n",
      " [-0.3137255  -0.3137255  -0.3137255  ... -0.3137255  -0.3137255\n",
      "  -0.3137255 ]\n",
      " [-0.3137255  -0.3137255  -0.3137255  ... -0.3137255  -0.3137255\n",
      "  -0.3137255 ]]\n",
      "(100, 128, 431)\n"
     ]
    }
   ],
   "source": [
    "train_data_normalized = train_data / 255.0  # Assuming pixel values range from 0 to 255\n",
    "test_data_normalized = test_data / 255.0\n",
    "print(train_data_normalized[0])\n",
    "print(train_data_normalized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "\n",
    "# Input shape\n",
    "input_shape = (128, 431, 1)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 63\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Use softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 679ms/step - loss: 4.2191 - accuracy: 0.0200 - val_loss: 4.1768 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 514ms/step - loss: 4.1382 - accuracy: 0.0400 - val_loss: 4.2589 - val_accuracy: 0.0385\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 518ms/step - loss: 4.1248 - accuracy: 0.0100 - val_loss: 4.6129 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 554ms/step - loss: 4.1586 - accuracy: 0.0000e+00 - val_loss: 4.4757 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 548ms/step - loss: 4.1087 - accuracy: 0.0200 - val_loss: 4.2436 - val_accuracy: 0.0385\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 516ms/step - loss: 4.1230 - accuracy: 0.0100 - val_loss: 4.2255 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 528ms/step - loss: 4.1219 - accuracy: 0.0200 - val_loss: 4.2583 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 2s 519ms/step - loss: 4.1142 - accuracy: 0.0200 - val_loss: 4.3223 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 2s 488ms/step - loss: 4.1042 - accuracy: 0.0300 - val_loss: 4.3721 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 2s 532ms/step - loss: 4.0992 - accuracy: 0.0200 - val_loss: 4.4284 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data_normalized, train_labels, epochs=10, validation_data=(test_data_normalized, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 262ms/step - loss: 4.4284 - accuracy: 0.0000e+00\n",
      "Mean Squared Error on Test Set: [4.428445816040039, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Assuming your model is already trained and saved in the 'model' variable\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "mse = model.evaluate(test_data_normalized, test_labels)\n",
    "\n",
    "# Print the MSE\n",
    "print(f'Mean Squared Error on Test Set: {mse}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
